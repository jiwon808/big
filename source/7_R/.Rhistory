#필요없는 문자, 단어 삭제하기
twitter$tw <- str_remove_all(twitter$tw,'\\W',' ')         #특수문자, space 빼라
#필요없는 문자, 단어 삭제하기
twitter$tw <- str_replace_all(twitter$tw,'\\W',' ')         #특수문자, space 빼라
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]',' ')
View(twitter)
View(twitter)
head(twitter)
nouns <- extractNoun(twitter$tw)
nouns
class(nouns)
nouns
wordcount <- table(unlist(nouns))
wordcount
class(wordcount)
df_word <- as.data.frame(wordcount,stringsAsFactor=FALSE)    #string이 factor변수가 아닌 문자형 변수로 들어감
df_word
str(df_word)
df_word <- as.data.frame(wordcount,stringsAsFactors=FALSE)    #string이 factor변수가 아닌 문자형 변수로 들어감
str(df_word)
df_word <- rename(df_word, word=Var1, freq=Freq)
head(df_word)
str(df_word)
# 출현단어 중 2글자 이상만 분석
df_word <- filter(df_word, nchar(word)>1)
head(df_word)
top20 <- df_word[order(df_word$freq, decreasing=T),][1:20,]
top20
df_word%>%
arrange(freq)%>%
head(20)%>%
ggplot()
df_word%>%
arrange(freq)%>%
head(20)%>%
ggplot(aes(x=freq,y=reorder(word,freq)))+
geom_col()
library(ggplot2)
df_word%>%
arrange(freq)%>%
head(20)%>%
ggplot(aes(x=freq,y=reorder(word,freq)))+
geom_col()
top20<-df_word%>%
arrange(desc(freq))%>%
head(20)
ggplot(top20,aes(x=freq,y=reorder(word,freq)))+
geom_col()+
coord_flip()
ggplot(top20,aes(x=freq,y=reorder(word,freq)))+
geom_col()+
geom_text(aes(label=freq),hjust=1, size=3, col='yellow')
#워드클라우드 그리기
set.seed(1234)
pal <- break.pal(9,'Blues')
pal <- break.pal(9,'Blues')[5:9]
pal <- brewer.pal(9,'Blues')[5:9]
wordcloud()
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq = 5,
max.words = 200,
random.order = F,     #최빈단어 가운데 나오게
rot.per = 0.1,        #????????????????
scale=c(3,0.3),
colors=pal
)
#
View(twitter)
#
library(dplyr)
library(KoNLP)
library(wordcloud)
library(ggplot2)
View(twitter)
library(stringr)
twitter <- read.csv('inData/twitter.csv',
header=TRUE,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
twitter <- rename(twitter,
no=번호, id=계정이름,date=작성일, tw=내용)
sort(table(twitter$id))
View(twitter)
View(twitter)
idCount <- as.data.frame(table(twitter$id))
twitter <- read.csv('inData/twitter.csv',
header=TRUE,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
twitter <- rename(twitter,
no=번호, id=계정이름,date=작성일, tw=내용)
sort(table(twitter$id))
idCount <- as.data.frame(table(twitter$id))
head(idCount)
idCount <- rename(idCount, id=Var1, count=Freq)
head(idCount)
twitter <- left_join(twitter,idCount,by="id")
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq =5,
max.words =200,
random.order = F,
rot.per=0.1,
scale=c(3,0.3),
colors=pal)
sampleId <- idCount[idCount$count<150, 'id']
sampleId
twitter1 <- subset(twitter,subset=id%in% sampleId)
twitter1
View(twitter1)
library(dplyr)
# 1. 정적 웹 크롤링: 단일 페이지 크롤링 (rvest 패키지 사용)
install.packages("rvest")
library(rvest)
text <- read_html(url,encoding='CP949')
text
#영화제목; .movie
nodes <- html_nodes(text,'.movie')
library(rvest)
url <- "https://movie.naver.com/movie/point/af/list.nhn"
text <- read_html(url,encoding='CP949')
text
#영화제목; .movie
nodes <- html_nodes(text,'.movie')
nodes
title
movieInfo <- html_attr(nodes,'href')
movieInfo <- url(html_attr(nodes,'href'))
#영화제목; .movie
nodes <- html_nodes(text,'.movie')    ########.movie는 사이트에 어디 있는지??
nodes
title <- html_text(nodes)
title
movieInfo <- url(html_attr(nodes,'href'))
movieInfo <- html_attr(nodes,'href')
movieInfo <- paste(url,movieInfo)
movieInfo
source('D:/bigdata(psa)/src/7_R/13장_웹데이터수집.R', encoding = 'UTF-8')
movieInfo
#해당 영화 페이지
movieInfo <- html_attr(nodes,'href')  ##########?????
movieInfo <- paste0(url,movieInfo)
movieInfo
#평점: .list_netizen_score em
nodes <- html_nodes(text,".list_netizen_score em")              #text중의 movie, list_netizen_score,  title을 가지고 온다
nodes
point <- html_text(nodes)
point
#영화 리뷰; .title
nodes <- html_nodes(text, '.title')
nodes
review <- html_text(nodes)
review
review <- gsub('\t','',review)
#3장 strsplit
review
#3장 strsplit
review <- gsub('\n','',review)
review
review <- html_text(nodes, trim=TRUE)
review <- gsub('\t','',review)
#3장 strsplit
review <- gsub('\n','',review)
review
strsplit(review,'중[0-9]{1,2}')
unlist(strsplit(review,'중[0-9]{1,2}'))      #####이부분 ?????????????
review<-unlist(strsplit(review,'중[0-9]{1,2}'))[seq(2,20,2)]    #2부터20까지 2 4 6 8 10 12 14 16 18 20    #####이부분 ?????????????
review<- gsub('신고','',review)
review
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page,review)
View(page)
# csv파일로 out
write.csv(page,"outData/movie_review.csv")
write.csv(page,"outData/movie_review.csv", row.names = F)
#영화제목; .movie
nodes <- html_nodes(text,'.movie')    ########.movie는 사이트에 어디 있는지???????????
nodes
customers <- c('kim@gmail.com,010-9999-8888,seoul Korea',
'yun@naver.com,02-716-1006,busan Korea',
'yun@naver.com,011-716-1006,busan Korea')
strsplit(customers,'[0-9]{2,3}-[0-9]{3,4}-[0-9]{4}') #******************복습
View(page)
# 1. 힙합 가사 텍스트 마이닝
#1.1 텍스트 마이닝 할 텍스트 로드 (필요한 data 확보)
txt <- readLines('inData/hiphop.txt') #비정형 데이터
temp<- gsub('\\W',' ',txt)     #gsub은 text replace         \\W  (꼭 대문자) (소문자면 숫자와 알파벳 뜻하고, 대문자W은 특수문자를 의미)               #########txt (앞에hiphop파일 읽어드림) 중 '특수문자'를 'space'로 바꿈################
txt <- str_replace_all(txt,'\\W',' ')
nouns <- extractNoun(txt)       #여기서 txt 앞에 'inData/hiphop.txt'를 가져옴
wordcount <- table(unlist(nouns)) # word count (단어별 빈도표)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)  #table를 data.frame으로 변환
str(df_word)
df_word <- rename(df_word, word=Var1, freq=Freq)  #이름변경   새로운 이름을 앞에다
head(df_word)
df_word <- df_word %>%
filter(nchar(word)>=2)  #nchar문자수 반환
head(df_word)
display.brewer.all()                    ####팔레트 #############
movie.review <- NULL
cat(url)
for(i in 1:100){
url <- paste0(site,i);
cat(url)
}
site = "https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=189141&target=after&page="
movie.review <- NULL
for(i in 1:100){
url <- paste0(site,i);
cat(url)
}
for(i in 1:100){
url <- paste0(site,i);
cat(url,"\n")   #"\n"해야 가독성이 높다
}
for(i in 1:100){
url <- paste0(site,i);
#cat(url,"\n")   #"\n"해야 가독성이 높다   #잘뿌려졌는지 확인용
text <- read_html(url,encoding='CP949')
#영화제목; .movie
nodes <- html_nodes(text,'.movie')    ########.movie는 사이트 F12 보면 나옴
title <- html_text(nodes)
#해당 영화 페이지
movieInfo <- html_attr(nodes,'href')  ##########nodes 중 href #################
movieInfo <- paste0(url,movieInfo)   ####  paste0          url링크 붙일 시 중간 space 없애줌
#평점: .list_netizen_score em
nodes <- html_nodes(text,".list_netizen_score em")              #text중의 movie, list_netizen_score,  title을 가지고 온다
point <- html_text(nodes)              #nodes는  'html_nodes(text,".list_netizen_score em")  ' 이 기때문에 해당 페이지의 평점을 가져옴
#영화 리뷰; .title
nodes <- html_nodes(text, '.title')
review <- html_text(nodes, trim=TRUE)
review <- gsub('\t','',review)
#3장 strsplit
review <- gsub('\n','',review)
review<-unlist(strsplit(review,'중[0-9]{1,2}'))[seq(2,20,2)]    #2부터20까지 2 4 6 8 10 12 14 16 18 20    #####이부분 3장 strsplit부분 참조
review<- gsub('신고','',review)   #신고를 space로 대체
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page,review)  #cbind는 2개 인자 밖에 못넣음. 옆에 다 column 추가
movie.review <- rbind(movie.review,page)
}
home <-"https://movie.naver.com/movie/point/af/list.nhn"
site <- "https://movie.naver.com/movie/point/af/list.nhn?st=mcode&sword=189141&target=after&page="
movie.review <- NULL
for(i in 1:100){
url <- paste0(site,i);
#cat(url,"\n")   #"\n"해야 가독성이 높다   #잘뿌려졌는지 확인용
text <- read_html(url,encoding='CP949')
#영화제목; .movie
nodes <- html_nodes(text,'.movie')    ########.movie는 사이트 F12 보면 나옴
title <- html_text(nodes)
#해당 영화 페이지
movieInfo <- html_attr(nodes,'href')  ##########nodes 중 href #################
movieInfo <- paste0(url,movieInfo)   ####  paste0          url링크 붙일 시 중간 space 없애줌
#평점: .list_netizen_score em
nodes <- html_nodes(text,".list_netizen_score em")              #text중의 movie, list_netizen_score,  title을 가지고 온다
point <- html_text(nodes)              #nodes는  'html_nodes(text,".list_netizen_score em")  ' 이 기때문에 해당 페이지의 평점을 가져옴
#영화 리뷰; .title
nodes <- html_nodes(text, '.title')
review <- html_text(nodes, trim=TRUE)
review <- gsub('\t','',review)
#3장 strsplit
review <- gsub('\n','',review)
review<-unlist(strsplit(review,'중[0-9]{1,2}'))[seq(2,20,2)]    #2부터20까지 2 4 6 8 10 12 14 16 18 20    #####이부분 3장 strsplit부분 참조
review<- gsub('신고','',review)   #신고를 space로 대체
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page,review)  #cbind는 2개 인자 밖에 못넣음. 옆에 다 column 추가
movie.review <- rbind(movie.review,page)
}
View(movie.review)
write.csv(movie.review,'outData/movie_review100page.csv')
class(movie.review)
home <-"https://movie.naver.com/movie/point/af/list.nhn"
site <- "https://movie.naver.com/movie/point/af/list.nhn?&page="
movie.review <- NULL
for(i in 1:100){
url <- paste0(site,i);
#cat(url,"\n")   #"\n"해야 가독성이 높다   #잘뿌려졌는지 확인용
text <- read_html(url,encoding='CP949')
#영화제목; .movie
nodes <- html_nodes(text,'.movie')    ########.movie는 사이트 F12 보면 나옴
title <- html_text(nodes)
#해당 영화 페이지
movieInfo <- html_attr(nodes,'href')  ##########nodes 중 href #################
movieInfo <- paste0(url,movieInfo)   ####  paste0          url링크 붙일 시 중간 space 없애줌
#평점: .list_netizen_score em
nodes <- html_nodes(text,".list_netizen_score em")              #text중의 movie, list_netizen_score,  title을 가지고 온다
point <- html_text(nodes)              #nodes는  'html_nodes(text,".list_netizen_score em")  ' 이 기때문에 해당 페이지의 평점을 가져옴
#영화 리뷰; .title
nodes <- html_nodes(text, '.title')
review <- html_text(nodes, trim=TRUE)
review <- gsub('\t','',review)
#3장 strsplit
review <- gsub('\n','',review)
review<-unlist(strsplit(review,'중[0-9]{1,2}'))[seq(2,20,2)]    #2부터20까지 2 4 6 8 10 12 14 16 18 20    #####이부분 3장 strsplit부분 참조
review<- gsub('신고','',review)   #신고를 space로 대체
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page,review)  #cbind는 2개 인자 밖에 못넣음. 옆에 다 column 추가
movie.review <- rbind(movie.review,page)
}
View(movie.review)
write.csv(movie.review,'outData/movie_review100page.csv')      #csv파일 만들어짐
library(dplyr)
library(KoNLP)
library(wordcloud)
movie <- as.data.frame(movie.review,stringsAsFactors = F)
str(movie)
movie$point <- as.numeric(movie$point)
movie$point <- as.numeric(movie$point)
result <- movie %>%
group_by(title)%>%
summarise(point.mean=mean(point),
point.sum= sum(point),
n=n())%>%
arrange(desc(point.mean), desc(point.sum))%>%
head(10)
ggplot(result,aes(x=point.sum,
y=reorder(title,point.sum)), vjust=1)+
geom_col(aes(fill=title))+
labs(title="평점이 높은 top10", x="평점 총점")+
geom_text(aes(label=paste('총점:', point.sum,'평균:', point.mean)), hjust=0)+
coord_cartesian(xlim=c(0,120))+
theme(axis.title.y = element_blank(),
legend.position = "none")
movie$reivew <- gsub('\\W', ' ', movie$review)
movie$reivew <- gsub('[ㄱ-ㅎ]' , movie$review)
movie$reivew <- gsub('[ㄱ-ㅎ]',' ', movie$review)
View(movie)
nouns <- extractNoun(movie$review)
wordcount <- table(unlist(nouns))
df_word <- as.data.frame(wordcount,stringsAsFactors = F)
df_word <- rename(df_word, word=Var1, freq=Freq)
View(df_word)
df_word <- filter(df_word,nchar(word)>1)
#최빈 단어 top20뽑고 그래프 그리기
top20 <- df_word[order(df_word$freq,decreasing =T),][1:20,]
top20
ggplot(top20, aes(x=freq,y=reorder(word,freq)))+
geom_col()+
geom_text(aes(label=freq),hjust=1,size=3, col='yellow')
#워드클라우드 그리기
set.seed(1234)
pal <- brewer.pal(8,'Dark2')
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq=5,
max.words = 200,
random.order = F,
rot.per=0.1,
scale=c(4,0.3),
colors=pal)
#현재 페이지의 html 소스 가져오기
html <- remD$getPageSource()[[1]]
remD <- remoteDriver(port=4445L,
browserName='chrome')
# 1. 정적 웹 크롤링: 단일 페이지 크롤링 (rvest 패키지 사용)
install.packages("rvest")
library(rvest)
#필요한 패키지 다운로드와 로드
install.packages("RSelenium")
library(httr)
library(rvest)
library(RSelenium)
remD <- remoteDriver(port=4445L,
browserName='chrome')
remD$navigate('https://youtu.be/tZooW6PritE')
remD$open() #서버를 브라우저가 연다
remD$navigate('https://youtu.be/tZooW6PritE')
btn <- remD$findElement(using='css selector',
value='.html5-main-video')
btn$clickElement()    #영상 플레이 멈춤
#마우스 스크롤 다운
remD$executeScript("window.scrollTo(0,500)")
remD$executeScript("window.scrollTo(500,1000)")
remD$executeScript("window.scrollTo(1000,1500)")
remD$executeScript("window.scrollTo(1000,3000)")
remD$executeScript("window.scrollTo(1000,5000)") ###########에러
#현재 페이지의 html 소스 가져오기
html <- remD$getPageSource()[[1]]
html <- read_html(html)
comments <- html%>%
html_nodes('#content-text')%>%
html_text()
comments[1:10]                       ############NA NA NA NA NA NA NA NA NA NA############
write.table(comments,
file='outData/댓글.txt',
sep=',',
#row.names=F,     #이거 안쓰면 '번호'가 잡힘
quote=FALSE)
result <- cbind(youtube_title,youtube_title_url)
#(1) 패키지 준비
install.packages("ggiraphExtra")
library(ggiraphExtra)
library(mapproj)
install.packages("mapproj") #ggChoropleth함수 사용을 위한 패키지
library(mapproj)
install.packages("maps")
install.packages("maps")
#미국 지도 그릴때, 미국 경도 위도    data파일필요
library(maps)
library(ggplot2)
library(tibble)
#(2) 행이름을 변수로
head(USArrests,1)
rownames_to_column(USArrests,var="state")
crime <- rownames_to_column(USArrests,var="state")
class(crime)
head(crime,3)
#대소문자 일치시켜야. 두파일의 state이름이 일치화되어 쓸 수 있다
crime$state <- tolower(crime$state) #주명을 소문자자
head(crime,3)
#(3) 미국 지도 주 정보 가져오기
map_data("state")
view(map_data())
view(map_data)
View( map_data("state") )
state<-map_data("state")
#(4) 지도 시각화
ggChoropleth()
#(4) 지도 시각화
ggChoropleth(data=crime, #지도에 표현할 데이터
aes(fill=Murder,
map_id=state),
map=state_map)
ggChoropleth(data=crime,        #지도에 표현할 데이터
aes(fill=Murder,   #지도에 채워질 변수
map_id=state), #지역 변수
map=state_map)
#(1) 패키지 준비
install.packages("ggiraphExtra") #지도 시각화를 위한 패키지
library(ggiraphExtra)
install.packages("mapproj") #ggChoropleth함수 사용을 위한 패키지
library(mapproj)
install.packages("maps") # map_data함수를 통해 지도 정보
#미국 지도 그릴때, 미국 경도 위도    data파일필요
library(maps)
library(ggplot2)
library(tibble) #행이름(지역명)을 변수로 하기 위한 작업
#(2) 행이름을 변수로
head(USArrests,1)
rownames_to_column(USArrests,var="state")
crime <- rownames_to_column(USArrests,var="state")
class(crime)
head(crime,3)
#대소문자 일치시켜야. 두파일의 state이름이 일치화되어 쓸 수 있다
crime$state <- tolower(crime$state) #주명을 소문자자
head(crime,3)
#(3) 미국 지도 주 정보 가져오기
map_data("state")
View( map_data("state") )
state<-map_data("state")
#(4) 지도 시각화
ggChoropleth(data=crime,        #지도에 표현할 데이터
aes(fill=Murder,   #지도에 채워질 변수
map_id=state), #지역 변수
map=state_map,     #위도 경도 지도 데이터
interactive=T)     # 인터렉티브 속성
state_map<-map_data("state")
#(4) 지도 시각화
ggChoropleth(data=crime,        #지도에 표현할 데이터
aes(fill=Murder,   #지도에 채워질 변수
map_id=state), #지역 변수
map=state_map,     #위도 경도 지도 데이터
interactive=T)     # 인터렉티브 속성
ggChoropleth(data=crime,        #지도에 표현할 데이터
aes(fill=Assault,   #지도에 채워질 변수
map_id=state), #지역 변수
map=state_map,     #위도 경도 지도 데이터
interactive=T)     # 인터렉티브 속성
# 2. 대한민국 시도별 인구, 결핵 환자 수 단계 구분도 만들기
rm(list=ls())
install.packages("stringi")
install.packages("stringi")
k
install.packages("devtools")
devtools::install_github("cardiomoon/kormaps2014")
library(kormaps2014)
head(korpop1)
str(changeCode(korpop1))
library(dplyr)
korpop1 <- rename(korpop1,
pop=총인구_명,
name=행정구역별_읍면동)
str(changeCode(korpop1))
head(kormap1)
str(changeCode(korpop1))  #utf-8로 인코딩 된 데이터를 CP949로 변환 후 출력
str(changeCode(kormap1))
head(changeCode(kormap1))
head(korpop1)
head(korpop1)
head(changeCode(korpop1[,c=('name','pop','code')]))
library(ggiraphExtra)
library(ggplot2)
head(changeCode(korpop1[,c=('name','pop','code')]))
korpop1$name <- iconv(korpop1$name, 'UTF-8','CP949')
korpop1$name
head(changeCode(korpop1[,c('name','pop','code')]))
#결핵환자수 지도 시각화
head(tbc)
head(changeCode(tbc))
tbc$name1 <- iconv(tbc$name1, 'UTF-8','CP949')
tbc$name <- iconv(tbc$name,'UTF-8','CP949')
head(tbc)
ggChoropleth(data=tbc,
aes(fill=NewPts,
map_id=code,
tooltip=name1),
map =kormap1,
interactive = T)
